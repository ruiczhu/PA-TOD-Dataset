#!/usr/bin/env python3
"""
ç®€åŒ–äººæ ¼å¯¹æ¯”ç»“æœå¯è§†åŒ–è„šæœ¬
ç”¨äºå¯è§†åŒ–è½¬æ¢åå¯¹è¯ä¸ä¸‰ä¸ªå‚è€ƒå€¼çš„å¯¹æ¯”ç»“æœ
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ComparisonResultsVisualizer:
    def __init__(self, results_path):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            results_path: å¯¹æ¯”ç»“æœCSVæ–‡ä»¶è·¯å¾„
        """
        self.results_path = results_path
        self.df = None
        self.output_dir = os.path.dirname(results_path)
        
    def load_results(self):
        """åŠ è½½å¯¹æ¯”ç»“æœ"""
        try:
            self.df = pd.read_csv(self.results_path)
            print(f"æˆåŠŸåŠ è½½ {len(self.df)} ä¸ªå¯¹è¯çš„å¯¹æ¯”ç»“æœ")
            return True
        except Exception as e:
            print(f"åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def plot_average_differences_comparison(self):
        """ç»˜åˆ¶ä¸‰ç§å¯¹æ¯”çš„å¹³å‡å·®å¼‚å¯¹æ¯”å›¾"""
        if self.df is None:
            print("è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        # å‡†å¤‡æ•°æ®
        comparison_types = ['vs_original', 'vs_target', 'vs_llm']
        comparison_labels = ['vs åŸå§‹å¯¹è¯', 'vs è½¬æ¢ç›®æ ‡å€¼', 'vs LLMè¯„åˆ†å€¼']
        avg_columns = ['avg_diff_vs_original', 'avg_diff_vs_target', 'avg_diff_vs_llm']
        
        avg_diffs = [self.df[col].mean() for col in avg_columns]
        std_diffs = [self.df[col].std() for col in avg_columns]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        bars = ax.bar(comparison_labels, avg_diffs, yerr=std_diffs, 
                     capsize=5, alpha=0.8, color=['skyblue', 'lightgreen', 'lightcoral'])
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, avg_diff in zip(bars, avg_diffs):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                   f'{avg_diff:.4f}', ha='center', va='bottom', fontweight='bold')
        
        ax.set_title('è½¬æ¢åå¯¹è¯æ¨¡å‹è¯„åˆ†çš„ä¸‰ç§å¯¹æ¯”å¹³å‡å·®å¼‚', fontsize=16, fontweight='bold')
        ax.set_ylabel('å¹³å‡ç»å¯¹å·®å¼‚', fontsize=12)
        ax.set_xlabel('å¯¹æ¯”ç±»å‹', fontsize=12)
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'average_differences_comparison.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_dimension_comparison_heatmap(self):
        """ç»˜åˆ¶å„ç»´åº¦å¯¹æ¯”å·®å¼‚çš„çƒ­åŠ›å›¾"""
        if self.df is None:
            print("è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        # å‡†å¤‡æ•°æ®
        dimensions = ['O', 'C', 'E', 'A', 'N']
        dimension_names = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨']
        comparison_types = ['vs_original', 'vs_target', 'vs_llm']
        comparison_labels = ['vs åŸå§‹å¯¹è¯', 'vs è½¬æ¢ç›®æ ‡å€¼', 'vs LLMè¯„åˆ†å€¼']
        
        # æ„å»ºçƒ­åŠ›å›¾æ•°æ®
        heatmap_data = []
        for comp_type in comparison_types:
            row = []
            for dim in dimensions:
                col_name = f'{dim}_{comp_type}'
                row.append(self.df[col_name].mean())
            heatmap_data.append(row)
        
        heatmap_df = pd.DataFrame(heatmap_data, 
                                 columns=dimension_names, 
                                 index=comparison_labels)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(heatmap_df, annot=True, fmt='.4f', cmap='YlOrRd', 
                   cbar_kws={'label': 'å¹³å‡ç»å¯¹å·®å¼‚'}, ax=ax)
        
        ax.set_title('å„äººæ ¼ç»´åº¦å¯¹æ¯”å·®å¼‚çƒ­åŠ›å›¾', fontsize=16, fontweight='bold')
        ax.set_xlabel('äººæ ¼ç»´åº¦', fontsize=12)
        ax.set_ylabel('å¯¹æ¯”ç±»å‹', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'dimension_comparison_heatmap.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_difference_distributions(self):
        """ç»˜åˆ¶ä¸‰ç§å¯¹æ¯”å·®å¼‚çš„åˆ†å¸ƒå›¾"""
        if self.df is None:
            print("è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        comparison_data = [
            ('avg_diff_vs_original', 'vs åŸå§‹å¯¹è¯', 'skyblue'),
            ('avg_diff_vs_target', 'vs è½¬æ¢ç›®æ ‡å€¼', 'lightgreen'),
            ('avg_diff_vs_llm', 'vs LLMè¯„åˆ†å€¼', 'lightcoral')
        ]
        
        for i, (col, label, color) in enumerate(comparison_data):
            axes[i].hist(self.df[col], bins=20, alpha=0.7, color=color, edgecolor='black')
            axes[i].set_title(f'{label}å·®å¼‚åˆ†å¸ƒ', fontsize=14)
            axes[i].set_xlabel('å¹³å‡ç»å¯¹å·®å¼‚', fontsize=12)
            axes[i].set_ylabel('é¢‘æ¬¡', fontsize=12)
            axes[i].grid(True, alpha=0.3)
            
            # æ·»åŠ ç»Ÿè®¡çº¿
            mean_val = self.df[col].mean()
            median_val = self.df[col].median()
            axes[i].axvline(mean_val, color='red', linestyle='--', label=f'å‡å€¼: {mean_val:.4f}')
            axes[i].axvline(median_val, color='green', linestyle='--', label=f'ä¸­ä½æ•°: {median_val:.4f}')
            axes[i].legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'difference_distributions.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_score_comparison_radar(self):
        """ç»˜åˆ¶äººæ ¼å¾—åˆ†å¯¹æ¯”é›·è¾¾å›¾ï¼ˆé€‰æ‹©ä¸€ä¸ªå¯¹è¯ä½œä¸ºç¤ºä¾‹ï¼‰"""
        if self.df is None:
            print("è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        # é€‰æ‹©å¹³å‡å·®å¼‚æœ€å¤§çš„å¯¹è¯ä½œä¸ºç¤ºä¾‹
        max_diff_idx = self.df['avg_diff_vs_target'].idxmax()
        sample_dialogue = self.df.iloc[max_diff_idx]
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        dimensions = ['O', 'C', 'E', 'A', 'N']
        dimension_names = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨']
        
        # å››ç§å¾—åˆ†
        original_scores = [sample_dialogue[f'original_model_{dim}'] for dim in dimensions]
        transformed_scores = [sample_dialogue[f'transformed_model_{dim}'] for dim in dimensions]
        target_scores = [sample_dialogue[f'target_{dim}'] for dim in dimensions]
        llm_scores = [sample_dialogue[f'llm_{dim}'] for dim in dimensions]
        
        # è®¾ç½®é›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(dimensions), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        # ç»˜åˆ¶å››æ¡çº¿
        def close_data(data):
            return data + data[:1]
        
        ax.plot(angles, close_data(original_scores), 'o-', linewidth=2, label='åŸå§‹å¯¹è¯æ¨¡å‹è¯„åˆ†', color='blue')
        ax.plot(angles, close_data(transformed_scores), 'o-', linewidth=2, label='è½¬æ¢åå¯¹è¯æ¨¡å‹è¯„åˆ†', color='red')
        ax.plot(angles, close_data(target_scores), 'o-', linewidth=2, label='è½¬æ¢ç›®æ ‡å€¼', color='green')
        ax.plot(angles, close_data(llm_scores), 'o-', linewidth=2, label='LLMè¯„åˆ†å€¼', color='orange')
        
        # å¡«å……è½¬æ¢åå¯¹è¯åŒºåŸŸ
        ax.fill(angles, close_data(transformed_scores), alpha=0.25, color='red')
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(dimension_names)
        ax.set_ylim(0, 1)
        ax.set_title(f'äººæ ¼å¾—åˆ†å¯¹æ¯”é›·è¾¾å›¾\n(å¯¹è¯ID: {sample_dialogue["dialogue_id"]})', 
                    fontsize=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'score_comparison_radar.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_correlation_analysis(self):
        """ç»˜åˆ¶ä¸åŒè¯„åˆ†ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æ"""
        if self.df is None:
            print("è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        dimensions = ['O', 'C', 'E', 'A', 'N']
        dimension_names = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, (dim, name) in enumerate(zip(dimensions, dimension_names)):
            ax = axes[i]
            
            # ç»˜åˆ¶è½¬æ¢åæ¨¡å‹è¯„åˆ†ä¸å…¶ä»–ä¸‰ç§è¯„åˆ†çš„ç›¸å…³æ€§
            transformed_col = f'transformed_model_{dim}'
            original_col = f'original_model_{dim}'
            target_col = f'target_{dim}'
            llm_col = f'llm_{dim}'
            
            # æ•£ç‚¹å›¾
            ax.scatter(self.df[target_col], self.df[transformed_col], 
                      alpha=0.6, label='vs è½¬æ¢ç›®æ ‡å€¼', color='green', s=30)
            ax.scatter(self.df[llm_col], self.df[transformed_col], 
                      alpha=0.6, label='vs LLMè¯„åˆ†å€¼', color='orange', s=30)
            ax.scatter(self.df[original_col], self.df[transformed_col], 
                      alpha=0.6, label='vs åŸå§‹å¯¹è¯', color='blue', s=30)
            
            # æ·»åŠ å¯¹è§’çº¿
            min_val = min(self.df[[transformed_col, original_col, target_col, llm_col]].min())
            max_val = max(self.df[[transformed_col, original_col, target_col, llm_col]].max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=1)
            
            ax.set_xlabel('å‚è€ƒè¯„åˆ†')
            ax.set_ylabel('è½¬æ¢åæ¨¡å‹è¯„åˆ†')
            ax.set_title(f'{name}ç›¸å…³æ€§åˆ†æ')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # ç§»é™¤å¤šä½™çš„å­å›¾
        axes[-1].remove()
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'correlation_analysis.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        if self.df is None:
            print("è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        print("\n" + "="*80)
        print("äººæ ¼è¯„åˆ†å¯¹æ¯”åˆ†ææ±‡æ€»æŠ¥å‘Š")
        print("="*80)
        
        print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {len(self.df)}")
        print(f"ğŸ”¢ å¹³å‡è½®æ¬¡æ•°: {self.df['turn_count'].mean():.2f}")
        
        # ä¸‰ç§å¯¹æ¯”çš„æ•´ä½“ç»Ÿè®¡
        print("\nä¸‰ç§å¯¹æ¯”çš„å¹³å‡å·®å¼‚ç»Ÿè®¡:")
        print("-" * 60)
        comparisons = [
            ('avg_diff_vs_original', 'è½¬æ¢å vs åŸå§‹å¯¹è¯'),
            ('avg_diff_vs_target', 'è½¬æ¢å vs è½¬æ¢ç›®æ ‡å€¼'),
            ('avg_diff_vs_llm', 'è½¬æ¢å vs LLMè¯„åˆ†å€¼')
        ]
        
        for col, name in comparisons:
            print(f"{name}:")
            print(f"  å¹³å‡å·®å¼‚: {self.df[col].mean():.4f}")
            print(f"  ä¸­ä½æ•°å·®å¼‚: {self.df[col].median():.4f}")
            print(f"  æ ‡å‡†å·®: {self.df[col].std():.4f}")
            print(f"  æœ€å°å·®å¼‚: {self.df[col].min():.4f}")
            print(f"  æœ€å¤§å·®å¼‚: {self.df[col].max():.4f}")
            print()
        
        # æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„å¯¹è¯
        best_vs_target_idx = self.df['avg_diff_vs_target'].idxmin()
        worst_vs_target_idx = self.df['avg_diff_vs_target'].idxmax()
        
        print("ä¸è½¬æ¢ç›®æ ‡å€¼å¯¹æ¯”ç»“æœ:")
        print(f"æœ€æ¥è¿‘ç›®æ ‡çš„å¯¹è¯: {self.df.loc[best_vs_target_idx, 'dialogue_id']} "
              f"(å·®å¼‚: {self.df.loc[best_vs_target_idx, 'avg_diff_vs_target']:.4f})")
        print(f"åç¦»ç›®æ ‡æœ€å¤§çš„å¯¹è¯: {self.df.loc[worst_vs_target_idx, 'dialogue_id']} "
              f"(å·®å¼‚: {self.df.loc[worst_vs_target_idx, 'avg_diff_vs_target']:.4f})")
    
    def create_all_visualizations(self):
        """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        if not self.load_results():
            return
        
        print("æ­£åœ¨ç”Ÿæˆå¯¹æ¯”åˆ†æå¯è§†åŒ–å›¾è¡¨...")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.generate_summary_report()
        
        # ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
        self.plot_average_differences_comparison()
        self.plot_dimension_comparison_heatmap()
        self.plot_difference_distributions()
        self.plot_score_comparison_radar()
        self.plot_correlation_analysis()
        
        print(f"\næ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {self.output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    results_path = "/Users/zhuruichen/PycharmProjects/PA-TOD-Dataset/output/mAPipelineOutput/rs_42_d_100/personality_comparison_results.csv"
    
    if not os.path.exists(results_path):
        print(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_path}")
        print("è¯·å…ˆè¿è¡Œ simplified_personality_comparator.py ç”Ÿæˆå¯¹æ¯”ç»“æœ")
        return
    
    print("äººæ ¼è¯„åˆ†å¯¹æ¯”ç»“æœå¯è§†åŒ–å·¥å…·")
    print("=" * 50)
    
    visualizer = ComparisonResultsVisualizer(results_path)
    visualizer.create_all_visualizations()


if __name__ == "__main__":
    main()
